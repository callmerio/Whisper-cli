#!/usr/bin/env python3
"""
Gemini éŸ³é¢‘è½¬å½•å™¨
ä½¿ç”¨ Gemini-2.5-Flash æ¨¡å‹è¿›è¡ŒéŸ³é¢‘è½¬å½•ï¼ŒåŸºäºå·²æœ‰çš„çº é”™å™¨å®ç°
"""

import concurrent.futures
import hashlib
import io
import json
import os
import tempfile
import threading
import time
import wave
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple

import numpy as np

try:
    from google import genai
    from google.genai import types
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸  google-genai æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: uv add google-genai")

import config


class GeminiTranscriber:
    """Gemini éŸ³é¢‘è½¬å½•å™¨ - åŸºäºçº é”™å™¨çš„å®ç°"""
    
    def __init__(self):
        """åˆå§‹åŒ– Gemini è½¬å½•å™¨"""
        self.api_key = config.GEMINI_TRANSCRIPTION_API_KEY
        self.model = config.GEMINI_TRANSCRIPTION_MODEL  # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
        self.is_ready = False
        self.last_run_info: Dict[str, Any] = {}
        self._prompt_cache: Optional[str] = None
        self._prompt_cache_mtime: Optional[Tuple[float, float]] = None
        
        if not GEMINI_AVAILABLE:
            print("âŒ Google Gen AI SDK æœªå®‰è£…")
            return
        
        # æ£€æŸ¥ API å¯†é’¥
        if not self.api_key:
            print("âŒ æœªæ‰¾åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡")
            return
        
        self.is_ready = bool(self.api_key)
        
        if self.is_ready:
            try:
                # é…ç½®ä»£ç† - ä½¿ç”¨ç¯å¢ƒå˜é‡æ–¹å¼ (ç…§æ¬çº é”™å™¨çš„å®ç°)
                if config.USE_PROXY:
                    proxy_url = config.HTTPS_PROXY if config.HTTPS_PROXY else config.HTTP_PROXY
                    if proxy_url:
                        # è®¾ç½®ç¯å¢ƒå˜é‡è®©åº•å±‚åº“ä½¿ç”¨ä»£ç†
                        os.environ['HTTP_PROXY'] = proxy_url
                        os.environ['HTTPS_PROXY'] = proxy_url
                        print(f"ğŸŒ ä½¿ç”¨ä»£ç†: {proxy_url}")
                
                # åˆ›å»ºGeminiå®¢æˆ·ç«¯ (æ”¯æŒè‡ªå®šä¹‰BASE_URL)
                http_options = None
                if config.GEMINI_BASE_URL:
                    # å¦‚æœé…ç½®äº†è‡ªå®šä¹‰BASE_URLï¼Œä½¿ç”¨å®ƒ
                    from google.genai import types
                    http_options = types.HttpOptions(base_url=config.GEMINI_BASE_URL)
                    print(f"ğŸ”— ä½¿ç”¨è‡ªå®šä¹‰BASE_URL: {config.GEMINI_BASE_URL}")
                
                self.client = genai.Client(api_key=self.api_key, http_options=http_options)
                
                print(f"âœ… Geminiè½¬å½•å™¨å·²å°±ç»ª ({self.model})")
                if config.DEBUG_MODE and self.api_key:
                    digest = hashlib.sha256(self.api_key.encode()).hexdigest()[:12]
                    print(f"   APIå¯†é’¥æŒ‡çº¹: {digest}")
                    
            except Exception as e:
                print(f"âŒ Geminiè½¬å½•å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.is_ready = False
        else:
            print("âš ï¸  Gemini APIå¯†é’¥æœªé…ç½®ï¼Œè½¬å½•åŠŸèƒ½ä¸å¯ç”¨")
            print(f"   è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½® GEMINI_API_KEY")
    
    def transcribe_complete_audio(self, audio_data: np.ndarray) -> Optional[str]:
        """
        è½¬å½•å®Œæ•´éŸ³é¢‘ - ä¼˜åŒ–ç‰ˆæœ¬æ”¯æŒå¤šç§ç­–ç•¥
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ® (numpy array)
            
        Returns:
            è½¬å½•æ–‡æœ¬æˆ– None
        """
        if not self.is_ready:
            print("âŒ Gemini è½¬å½•å™¨æœªå‡†å¤‡å°±ç»ª")
            return None
        
        if audio_data is None or len(audio_data) == 0:
            print("âŒ éŸ³é¢‘æ•°æ®ä¸ºç©º")
            return None
        
        audio_duration = len(audio_data) / config.SAMPLE_RATE
        self.last_run_info = {
            "audio_duration_s": audio_duration,
            "sample_count": len(audio_data),
            "model": self.model,
            "strategy": "single",
            "compressed_kb": None,
            "transcript_chars": 0,
            "api_attempts": 0,
            "payload_type": None,
            "chunks_total": 0,
            "chunks_success": 0,
        }
        print(f"ğŸ“Š éŸ³é¢‘æ—¶é•¿: {audio_duration:.1f}ç§’")

        # æ ¹æ®éŸ³é¢‘é•¿åº¦é€‰æ‹©å¤„ç†ç­–ç•¥
        if audio_duration <= 30:  # çŸ­éŸ³é¢‘ï¼šç›´æ¥å¤„ç†
            return self._transcribe_single_audio(audio_data)
        elif audio_duration <= 120:  # ä¸­ç­‰é•¿åº¦ï¼šå‹ç¼©ä¼˜åŒ–
            self.last_run_info["strategy"] = "compressed"
            return self._transcribe_compressed_audio(audio_data)
        else:  # é•¿éŸ³é¢‘ï¼šåˆ†ç‰‡å¹¶è¡Œå¤„ç†
            self.last_run_info["strategy"] = "chunked"
            return self._transcribe_chunked_audio(audio_data)
    
    def _transcribe_single_audio(self, audio_data: np.ndarray) -> Optional[str]:
        """å¤„ç†çŸ­éŸ³é¢‘ï¼ˆâ‰¤30ç§’ï¼‰"""
        try:
            # åˆ›å»ºå‹ç¼©çš„éŸ³é¢‘æ•°æ®
            audio_bytes = self._create_compressed_audio_bytes(audio_data)
            if not audio_bytes:
                return None
            
            compressed_kb = len(audio_bytes) / 1024
            self.last_run_info["compressed_kb"] = compressed_kb
            self.last_run_info["payload_type"] = "single"
            print(f"ğŸ“ å‹ç¼©éŸ³é¢‘å¤§å°: {compressed_kb:.1f}KB")

            # ç›´æ¥è°ƒç”¨APIï¼Œæ— éœ€ä¸´æ—¶æ–‡ä»¶
            prompt_text = self._build_prompt()

            transcript = self._call_gemini_audio_api_bytes(audio_bytes, prompt_text)

            if transcript:
                self.last_run_info["transcript_chars"] = len(transcript)
                print(f"âœ… Gemini è½¬å½•å®Œæˆ: {len(transcript)} å­—ç¬¦")
                return transcript
            else:
                print("âŒ Gemini è½¬å½•å¤±è´¥")
                return None
                
        except Exception as e:
            print(f"âŒ Gemini è½¬å½•é”™è¯¯: {e}")
        
        return None
    
    def _transcribe_compressed_audio(self, audio_data: np.ndarray) -> Optional[str]:
        """å¤„ç†ä¸­ç­‰é•¿åº¦éŸ³é¢‘ï¼ˆ30-120ç§’ï¼‰- ä½¿ç”¨éŸ³é¢‘å‹ç¼©"""
        try:
            # é™é‡‡æ ·ä»¥å‡å°‘æ–‡ä»¶å¤§å°
            if config.SAMPLE_RATE > 16000:
                # é™é‡‡æ ·åˆ°16kHz
                downsample_factor = config.SAMPLE_RATE // 16000
                if downsample_factor > 1:
                    audio_data = audio_data[::downsample_factor]
                    print(f"ğŸ“‰ é™é‡‡æ ·: {config.SAMPLE_RATE}Hz â†’ 16000Hz")
            
            # éŸ³é‡æ ‡å‡†åŒ–
            max_val = np.max(np.abs(audio_data))
            if max_val > 0:
                audio_data = (audio_data / max_val * 0.9).astype(audio_data.dtype)
            
            # åˆ›å»ºå‹ç¼©éŸ³é¢‘
            audio_bytes = self._create_compressed_audio_bytes(audio_data, sample_rate=16000)
            if not audio_bytes:
                return None
            
            compressed_kb = len(audio_bytes) / 1024
            self.last_run_info["compressed_kb"] = compressed_kb
            self.last_run_info["payload_type"] = "compressed"
            print(f"ğŸ“ å‹ç¼©éŸ³é¢‘å¤§å°: {compressed_kb:.1f}KB")

            # è°ƒç”¨API
            prompt_text = self._build_prompt()

            transcript = self._call_gemini_audio_api_bytes(audio_bytes, prompt_text)

            if transcript:
                self.last_run_info["transcript_chars"] = len(transcript)
                print(f"âœ… Gemini è½¬å½•å®Œæˆ: {len(transcript)} å­—ç¬¦")
                return transcript
            else:
                print("âŒ Gemini è½¬å½•å¤±è´¥")
                return None
                
        except Exception as e:
            print(f"âŒ Gemini è½¬å½•é”™è¯¯: {e}")
        
        return None
    
    def _transcribe_chunked_audio(self, audio_data: np.ndarray) -> Optional[str]:
        """å¤„ç†é•¿éŸ³é¢‘ï¼ˆ>120ç§’ï¼‰- åˆ†ç‰‡å¹¶è¡Œå¤„ç†"""
        try:
            chunk_size = 60 * config.SAMPLE_RATE  # 60ç§’åˆ†ç‰‡
            chunks = []
            
            # åˆ†ç‰‡å¤„ç†
            for i in range(0, len(audio_data), chunk_size):
                chunk = audio_data[i:i + chunk_size]
                if len(chunk) > config.SAMPLE_RATE:  # è‡³å°‘1ç§’
                    chunks.append((i // config.SAMPLE_RATE, chunk))
            
            self.last_run_info["chunks_total"] = len(chunks)
            print(f"ğŸ“Š åˆ†ç‰‡å¤„ç†: {len(chunks)} ä¸ªç‰‡æ®µ")
            
            # å¹¶è¡Œè½¬å½•
            transcripts = []
            prompt_text = self._build_prompt()
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                future_to_chunk = {
                    executor.submit(self._transcribe_chunk, chunk_id, chunk_data, prompt_text): chunk_id 
                    for chunk_id, chunk_data in chunks
                }
                
                for future in concurrent.futures.as_completed(future_to_chunk):
                    chunk_id = future_to_chunk[future]
                    try:
                        result = future.result()
                        if result:
                            transcripts.append((chunk_id, result))
                    except Exception as e:
                        print(f"âŒ åˆ†ç‰‡ {chunk_id} è½¬å½•å¤±è´¥: {e}")
            
            # æŒ‰é¡ºåºåˆå¹¶ç»“æœ
            transcripts.sort(key=lambda x: x[0])
            final_transcript = ' '.join([t[1] for t in transcripts])
            
            if final_transcript:
                self.last_run_info["transcript_chars"] = len(final_transcript)
                self.last_run_info["chunks_success"] = len(transcripts)
                self.last_run_info["payload_type"] = "chunked"
                print(f"âœ… åˆ†ç‰‡è½¬å½•å®Œæˆ: {len(transcripts)}/{len(chunks)} ä¸ªç‰‡æ®µæˆåŠŸ")
                return final_transcript
            else:
                print("âŒ æ‰€æœ‰åˆ†ç‰‡è½¬å½•å¤±è´¥")
                return None
                
        except Exception as e:
            print(f"âŒ åˆ†ç‰‡è½¬å½•é”™è¯¯: {e}")
        
        return None
    
    def _transcribe_chunk(self, chunk_id: int, chunk_data: np.ndarray, prompt_text: Optional[str]) -> Optional[str]:
        """è½¬å½•å•ä¸ªéŸ³é¢‘åˆ†ç‰‡"""
        try:
            # å‹ç¼©åˆ†ç‰‡
            audio_bytes = self._create_compressed_audio_bytes(chunk_data, sample_rate=16000)
            if not audio_bytes:
                return None
            
            # è°ƒç”¨API
            return self._call_gemini_audio_api_bytes(audio_bytes, prompt_text)
            
        except Exception as e:
            print(f"âŒ åˆ†ç‰‡ {chunk_id} å¤„ç†å¤±è´¥: {e}")
            return None

    def check_health(self) -> Tuple[bool, str]:
        """æ£€æŸ¥ Gemini æ¥å£æ˜¯å¦å¯ç”¨"""
        if not self.is_ready:
            return False, "Gemini è½¬å½•å™¨æœªåˆå§‹åŒ–"

        if not hasattr(self, 'client'):
            return False, "Gemini å®¢æˆ·ç«¯æœªåˆ›å»º"

        try:
            contents = [
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text="health check ping")]
                )
            ]
            config = types.GenerateContentConfig(
                response_mime_type="text/plain",
                temperature=0.0,
                max_output_tokens=8,
            )
            self.client.models.generate_content(
                model=self.model,
                contents=contents,
                config=config,
            )
            return True, ""
        except Exception as exc:
            return False, str(exc)
    
    def _call_gemini_audio_api_bytes(self, audio_bytes: bytes, prompt_text: Optional[str] = None) -> Optional[str]:
        """ç›´æ¥ä½¿ç”¨éŸ³é¢‘å­—èŠ‚æ•°æ®è°ƒç”¨Gemini API"""
        for attempt in range(config.GEMINI_MAX_RETRIES):
            try:
                print(f"ğŸ”„ APIè°ƒç”¨å°è¯• {attempt + 1}/{config.GEMINI_MAX_RETRIES}")
                
                # æ„å»ºå†…å®¹
                base_prompt = prompt_text or config.GEMINI_TRANSCRIPTION_PROMPT

                contents = [
                    types.Content(
                        role="user",
                        parts=[
                            types.Part.from_text(text=base_prompt),
                            types.Part.from_bytes(
                                data=audio_bytes,
                                mime_type="audio/wav"
                            )
                        ],
                    ),
                ]
                
                # ç”Ÿæˆé…ç½®ï¼Œä¼˜åŒ–å‚æ•°
                thinking_budget = config.GEMINI_THINKING_BUDGET
                generate_content_config = types.GenerateContentConfig(
                    thinking_config=types.ThinkingConfig(
                        thinking_budget=thinking_budget,
                    ),
                    response_mime_type="text/plain",
                    temperature=0.0,  # æ›´ä½æ¸©åº¦ï¼Œæ›´å¿«å“åº”
                    max_output_tokens=1000,  # å‡å°‘è¾“å‡ºtokené™åˆ¶
                )

                # APIè°ƒç”¨
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=contents,
                    config=generate_content_config,
                )
                
                if response and response.text:
                    self.last_run_info["api_attempts"] = attempt + 1
                    return response.text.strip()
                else:
                    print(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼šAPIè¿”å›ç©ºå“åº”")
                    
            except Exception as e:
                error_msg = str(e)
                print(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {error_msg}")
                
                # åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
                if attempt < config.GEMINI_MAX_RETRIES - 1:
                    if "UPSTREAM_ERROR" in error_msg or "EOF" in error_msg or "500" in error_msg:
                        print(f"ğŸ”„ æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œ{config.GEMINI_RETRY_DELAY}ç§’åé‡è¯•...")
                        time.sleep(config.GEMINI_RETRY_DELAY)
                        continue
                    elif "timeout" in error_msg.lower():
                        print(f"â° è¯·æ±‚è¶…æ—¶ï¼Œ{config.GEMINI_RETRY_DELAY}ç§’åé‡è¯•...")
                        time.sleep(config.GEMINI_RETRY_DELAY)
                        continue
                    else:
                        # ä¸å¯é‡è¯•çš„é”™è¯¯
                        print(f"âŒ ä¸å¯é‡è¯•çš„é”™è¯¯: {error_msg}")
                        break
                else:
                    print(f"âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
        
        self.last_run_info["api_attempts"] = config.GEMINI_MAX_RETRIES
        return None
    
    def _call_gemini_audio_api(self, audio_file_path: str) -> Optional[str]:
        """è°ƒç”¨GeminiéŸ³é¢‘APIï¼ŒåŒ…å«é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†"""
        for attempt in range(config.GEMINI_MAX_RETRIES):
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(audio_file_path)
                print(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶å¤§å°: {file_size / 1024:.1f}KB")
                
                if file_size > config.GEMINI_AUDIO_MAX_SIZE:
                    print(f"âŒ éŸ³é¢‘æ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.1f}MB (æœ€å¤§: {config.GEMINI_AUDIO_MAX_SIZE / 1024 / 1024}MB)")
                    return None
                
                # è¯»å–éŸ³é¢‘æ–‡ä»¶
                with open(audio_file_path, 'rb') as f:
                    audio_data_bytes = f.read()
                
                print(f"ğŸ”„ APIè°ƒç”¨å°è¯• {attempt + 1}/{config.GEMINI_MAX_RETRIES}")
                
                # æ„å»ºå†…å®¹
                base_prompt = self._build_prompt()

                contents = [
                    types.Content(
                        role="user",
                        parts=[
                            types.Part.from_text(text=base_prompt),
                            types.Part.from_bytes(
                                data=audio_data_bytes,
                                mime_type="audio/wav"
                            )
                        ],
                    ),
                ]
                
                # ç”Ÿæˆé…ç½®ï¼Œæ”¯æŒåŠ¨æ€ thinking è®¾ç½®
                thinking_budget = config.GEMINI_THINKING_BUDGET
                generate_content_config = types.GenerateContentConfig(
                    thinking_config=types.ThinkingConfig(
                        thinking_budget=thinking_budget,
                    ),
                    response_mime_type="text/plain",
                    temperature=0.1,
                    max_output_tokens=2000,
                )

                # APIè°ƒç”¨
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=contents,
                    config=generate_content_config,
                )
                
                if response and response.text:
                    return response.text.strip()
                else:
                    print(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼šAPIè¿”å›ç©ºå“åº”")
                    
            except Exception as e:
                error_msg = str(e)
                print(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {error_msg}")
                
                # åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
                if attempt < config.GEMINI_MAX_RETRIES - 1:
                    if "UPSTREAM_ERROR" in error_msg or "EOF" in error_msg or "500" in error_msg:
                        print(f"ğŸ”„ æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œ{config.GEMINI_RETRY_DELAY}ç§’åé‡è¯•...")
                        time.sleep(config.GEMINI_RETRY_DELAY)
                        continue
                    elif "timeout" in error_msg.lower():
                        print(f"â° è¯·æ±‚è¶…æ—¶ï¼Œ{config.GEMINI_RETRY_DELAY}ç§’åé‡è¯•...")
                        time.sleep(config.GEMINI_RETRY_DELAY)
                        continue
                    else:
                        # ä¸å¯é‡è¯•çš„é”™è¯¯
                        print(f"âŒ ä¸å¯é‡è¯•çš„é”™è¯¯: {error_msg}")
                        break
                else:
                    print(f"âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
        
        return None
    
    def _create_compressed_audio_bytes(self, audio_data: np.ndarray, sample_rate: Optional[int] = None) -> Optional[bytes]:
        """
        åˆ›å»ºå‹ç¼©çš„éŸ³é¢‘å­—èŠ‚æ•°æ®ï¼ˆç›´æ¥åœ¨å†…å­˜ä¸­å¤„ç†ï¼Œæ— éœ€ä¸´æ—¶æ–‡ä»¶ï¼‰
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            sample_rate: é‡‡æ ·ç‡ï¼ˆå¯é€‰ï¼Œç”¨äºé™é‡‡æ ·ï¼‰
            
        Returns:
            WAVæ ¼å¼çš„å­—èŠ‚æ•°æ®æˆ– None
        """
        try:
            # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯æ­£ç¡®çš„æ ¼å¼
            if audio_data.dtype != np.int16:
                if audio_data.dtype == np.float32 or audio_data.dtype == np.float64:
                    # æµ®ç‚¹æ ¼å¼è½¬æ¢
                    audio_data = (audio_data * 32767).astype(np.int16)
                else:
                    audio_data = audio_data.astype(np.int16)
            
            # ä½¿ç”¨å†…å­˜ç¼“å†²åŒºåˆ›å»ºWAV
            buffer = io.BytesIO()
            
            # å†™å…¥ WAV æ•°æ®
            with wave.open(buffer, 'wb') as wav_file:
                wav_file.setnchannels(config.CHANNELS)
                wav_file.setsampwidth(2)  # 16-bit
                wav_file.setframerate(sample_rate or config.SAMPLE_RATE)
                wav_file.writeframes(audio_data.tobytes())
            
            # è·å–å­—èŠ‚æ•°æ®
            audio_bytes = buffer.getvalue()
            buffer.close()
            
            return audio_bytes
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå‹ç¼©éŸ³é¢‘æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _save_audio_to_temp_file(self, audio_data: np.ndarray) -> Optional[str]:
        """
        å°†éŸ³é¢‘æ•°æ®ä¿å­˜ä¸ºä¸´æ—¶ WAV æ–‡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            
        Returns:
            ä¸´æ—¶æ–‡ä»¶è·¯å¾„æˆ– None
        """
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            fd, temp_path = tempfile.mkstemp(suffix='.wav', prefix='gemini_audio_')
            os.close(fd)
            
            # ä½¿ç”¨å‹ç¼©æ–¹æ³•åˆ›å»ºéŸ³é¢‘æ•°æ®
            audio_bytes = self._create_compressed_audio_bytes(audio_data)
            if not audio_bytes:
                return None
            
            # å†™å…¥æ–‡ä»¶
            with open(temp_path, 'wb') as f:
                f.write(audio_bytes)
            
            print(f"ğŸ“ ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶: {temp_path} ({os.path.getsize(temp_path) / 1024:.1f}KB)")
            return temp_path
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def stop_processing(self):
        """åœæ­¢å¤„ç†ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        # Gemini æ˜¯è¯·æ±‚-å“åº”æ¨¡å¼ï¼Œæ— éœ€ç‰¹æ®Šåœæ­¢æ“ä½œ
        pass
    
    def get_supported_formats(self) -> List[str]:
        """è·å–æ”¯æŒçš„éŸ³é¢‘æ ¼å¼"""
        return config.GEMINI_AUDIO_FORMATS.copy()

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "name": "Gemini Audio Transcriber",
            "model": self.model,
            "language": config.GEMINI_AUDIO_LANGUAGE,
            "max_audio_size": config.GEMINI_AUDIO_MAX_SIZE,
            "supported_formats": self.get_supported_formats(),
            "is_ready": self.is_ready
        }

    # ==================== Prompt æ„å»º ====================

    def _build_prompt(self) -> str:
        """ç”ŸæˆåŒ…å«è¯å…¸ä¸çº é”™è®°å¿†çš„æç¤ºè¯"""

        inject_dict = getattr(config, "INJECT_DICTIONARY_IN_PROMPT", True)
        inject_corr = getattr(config, "INJECT_CORRECTIONS_IN_PROMPT", True)
        inject_history = getattr(config, "INJECT_HISTORY_IN_PROMPT", True)

        if not any([inject_dict, inject_corr, inject_history]):
            return config.GEMINI_TRANSCRIPTION_PROMPT

        cache_key = self._get_prompt_cache_key(inject_dict, inject_corr, inject_history)
        if (
            cache_key == self._prompt_cache_mtime
            and self._prompt_cache is not None
        ):
            return self._prompt_cache

        parts: List[str] = [config.GEMINI_TRANSCRIPTION_PROMPT.strip()]

        if inject_dict:
            dict_section = self._load_dictionary_section()
            if dict_section:
                header = getattr(
                    config,
                    "PROMPT_DICTIONARY_HEADER",
                    "è¯æ±‡åå¥½ï¼ˆç™¾åˆ†æ¯”ä¸ºå‚è€ƒæƒé‡ï¼‰ï¼š",
                )
                parts.append(f"{header}\n{dict_section}")

        if inject_corr:
            corr_section = self._load_corrections_section()
            if corr_section:
                header = getattr(
                    config,
                    "PROMPT_CORRECTIONS_HEADER",
                    "ç¤ºä¾‹çº é”™ï¼ˆç”¨äºå‚è€ƒï¼‰ï¼š",
                )
                parts.append(f"{header}\n{corr_section}")

        if inject_history:
            history_section = self._load_history_section()
            if history_section:
                header = getattr(
                    config,
                    "PROMPT_HISTORY_HEADER",
                    "è¿‘æœŸä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆä»…ä¾›å‚è€ƒï¼‰ï¼š",
                )
                parts.append(f"{header}\n{history_section}")

        prompt_text = "\n\n".join(parts)
        if getattr(config, "PRINT_PROMPT_ON_TRANSCRIBE", False):
            print("\n===== PROMPT START =====")
            print(prompt_text)
            print("===== PROMPT END =====\n")
        self._prompt_cache = prompt_text
        self._prompt_cache_mtime = cache_key
        return prompt_text

    def _get_prompt_cache_key(
        self, inject_dict: bool, inject_corr: bool, inject_history: bool
    ) -> Tuple[float, float, float]:
        dictionary_path = Path(config.DICTIONARY_FILE)
        corrections_path = Path(config.PROJECT_ROOT) / "logs" / "corrections.txt"
        history_path = Path(config.PROJECT_ROOT) / "logs" / "history.json"

        dict_mtime = dictionary_path.stat().st_mtime if (inject_dict and dictionary_path.exists()) else 0.0
        corr_mtime = corrections_path.stat().st_mtime if (inject_corr and corrections_path.exists()) else 0.0
        history_mtime = history_path.stat().st_mtime if (inject_history and history_path.exists()) else 0.0
        return dict_mtime, corr_mtime, history_mtime

    def _load_dictionary_section(self) -> str:
        dictionary_path = Path(config.DICTIONARY_FILE)
        if not dictionary_path.exists():
            return ""
        try:
            lines: List[str] = []
            with dictionary_path.open("r", encoding="utf-8") as f:
                for line in f:
                    stripped = line.rstrip("\n")
                    if not stripped or stripped.lstrip().startswith("#"):
                        continue
                    lines.append(stripped)
            return "\n".join(lines)
        except Exception as exc:
            if config.DEBUG_MODE:
                print(f"âš ï¸ è¯»å–è¯å…¸ç”¨äºæç¤ºè¯å¤±è´¥: {exc}")
            return ""

    def _load_corrections_section(self) -> str:
        corrections_path = Path(config.PROJECT_ROOT) / "logs" / "corrections.txt"
        if not corrections_path.exists():
            return ""
        try:
            content = corrections_path.read_text(encoding="utf-8")
            return content.strip()
        except Exception as exc:
            if config.DEBUG_MODE:
                print(f"âš ï¸ è¯»å– corrections.txt å¤±è´¥: {exc}")
            return ""

    def _load_history_section(self) -> str:
        history_path = Path(config.PROJECT_ROOT) / "logs" / "history.json"
        if not history_path.exists():
            return ""

        try:
            raw = history_path.read_text(encoding="utf-8")
            history = json.loads(raw) if raw.strip() else []
        except Exception as exc:
            if config.DEBUG_MODE:
                print(f"âš ï¸ è¯»å– history.json å¤±è´¥: {exc}")
            return ""

        if not isinstance(history, list):
            return ""

        limit = max(1, getattr(config, "PROMPT_HISTORY_LIMIT", 2))
        max_chars = max(50, getattr(config, "PROMPT_HISTORY_MAX_CHARS", 300))

        recent_texts: List[str] = []
        for entry in history:
            if not isinstance(entry, dict):
                continue
            text = (entry.get("text") or "").strip()
            if not text:
                continue
            if len(text) > max_chars:
                text = text[: max_chars - 1].rstrip() + "â€¦"
            recent_texts.append(text)

        if not recent_texts:
            return ""

        selected = recent_texts[-limit:]
        lines = [f"{idx}. {value}" for idx, value in enumerate(selected, start=1)]
        return "\n".join(lines)


def test_gemini_transcriber():
    """æµ‹è¯• Gemini è½¬å½•å™¨"""
    print("ğŸ§ª æµ‹è¯• Gemini è½¬å½•å™¨...")
    
    transcriber = GeminiTranscriber()
    
    if not transcriber.is_ready:
        print("âŒ è½¬å½•å™¨æœªå‡†å¤‡å°±ç»ª")
        return False
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    info = transcriber.get_model_info()
    print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    print("âœ… Gemini è½¬å½•å™¨æµ‹è¯•å®Œæˆ")
    return True


if __name__ == "__main__":
    test_gemini_transcriber()
